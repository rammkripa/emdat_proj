---
title: "Weed: A package Geocoding The EMDAT Dataset"
author: "Ram Mukund Kripa"
date: "7/15/2021"
output:
  word_document: default
  md_document : default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
```

```{r packages, include=FALSE}
library(weed)
library(tidyverse)
library(here)
library(gt)
library(sf)
library(sp)
library(USAboundaries)
library(flextable)
```

## Abstract

EM-DAT is a very popular disaster dataset among researchers in the field of Earth and Environmental science. However, it comes with challenges, in its format and otherwise, that make it unsuitable to a tidy data science workflow in popular environments like R. The Wrangler for Emergency Events Database, weed for short, aims to provide researchers with the right tools to easily prepare EM-DAT data for downstream analysis, as well as some exploration of the same. The sample workflow here demonstrates the capabilities of weed through its functions.



## Keywords

## Introduction

WEED (Wrangler for Emergency Events Database) is an R Package designed to help researches in the field of climatological disaster studies better analyse EM-DAT and other related datasets. As will be explained in later sections of this paper, EM-DAT contains a lot of useful climatoligical and sociological data pertaining to natural disasters. However, it has a few drawbacks in its presentation of geolocation data as well as its unsuitability to a tidy data science workflow. 

WEED attempts to solve and eradicate these issues to help researchers glean valuable insights from their EM-DAT dataset!

## Structure of The Package

The structure of the Wrangler for Emergency Events Database is best explained through a sample workflow.

### Loading

First, one may download the subset of EM-DAT required from the public EM-DAT query tool located at [this link](https://public.emdat.be/). From here, researchers can exploit the loading functionality of WEED. EM-DAT files tend to be excel sheets with a few lines of metadata, followed by the subsection of the dataset downloaded. The read_emdat function allows researchers to load the dataset as a List in one of two ways: the dataset, as well as its metadata, or just the dataset.

```{r, results=FALSE}
sample_data <- read_emdat(here("data", "emdat_public_2021_01_12_full.xlsx"))
summary(sample_data)
```

```{r viewemdat, echo=FALSE}
sample_data[['disaster_data']] %>%
  head() %>%
  select('Dis No', 'Country', 'Location', 'Year') %>%
  data.frame() %>%
  flextable() %>%
  add_header_row(values = c("Sample Data"), colwidths = c(4))
```

### Exploration

The next step might be to explore the given data. One of the most pressing issues with EM-DAT at the moment is its geocoding data. The fact that a very small fraction of disasters have usable geocoding data, in terms of Latitude and Longitude, severely hampers location analysis. Another issue is the presence of multiple locations per disaster. 

```{r, results=FALSE, echo = FALSE, warning = FALSE}
sample_df <- read_csv(here("sample_data.csv")) %>%
  select(`Dis No`, Country, `Disaster Type`, Location, Latitude, Longitude) %>%
  filter(!is.na(`Dis No`))

```


```{r view_sampledf, echo=FALSE}
sample_df %>%
  data.frame() %>%
  flextable() %>%
  add_header_row(values = c("Sample Data: Problems"), colwidths = c(6)) %>%
  bg(j = c("Latitude", "Longitude"), bg = "cadetblue1") %>%
  bg(j = c("Location"), bg = "coral")
```

Having a single latitude and longitude refer to multiple locations also makes analysis significantly more challenging, and nigh on impossible. To counteract these problems, the recommended Weed workflow is to change the unit of analysis of the data frame from “one row per disaster” to “one row per disaster-location pair”. This process will henceforth be referred to as “locationizing”.

```{r}
locationized_df <- sample_df %>%
  split_locations(column_name = "Location")
```

```{r view_locationized, echo=FALSE}
locationized_df %>%
  head() %>%
  select(-Location) %>%
  flextable() %>%
  add_header_row(values = c('Locationized Sample Data'),colwidths = c(7)) %>%
  bg(j = c("location_word"), bg = "blueviolet") %>%
  bg(j = c("Latitude", "Longitude"), bg = "cadetblue1") %>%
  bg(j = c("Dis No"), bg = "gold") %>%
  bg(j = c('uncertain_location_specificity'), bg = scales::col_factor(palette = c("green", "pink"), domain = NULL))
```

The split_locations function allows users to execute the process of “locationizing”. It comes with a default method of splitting, defined by its parameters ‘dummy_words’, which indicate which words to altogether remove from the location strings, and ‘joiner_regex’, which indicates how the locations have been concatenated to form the location strings.

A few notes on the kinds of locations we get:

Collections of locations are represented in a wide variety of ways in EM-DAT, owing to the fact that this is a dataset that is manually filled row-by-row. Since there is only one location column, a solitary string is associated with each disaster, and there is a need for a delimiter to distinguish between separate locations affected by the disaster. Common methods of representation of abritrary locations (here called A, B and C) include "A, B, and C", "(1) A (2) B (3) C" and "A (B and C)." Such delimiters are handled by the location extraction algorithm, but others may be accounted for using the joiner_regex parameter.

When parentheses are present in location strings, like "Berkeley (California)" or "California (Berkeley, Emeryville, Alameda)", we see that problems can arise in regard to differing levels of specificity. We are unsure of the specificity of each these locations just by their position relative to their associated parentheses. Hence, we flag these with an uncertainty column called "uncertain_location_specificity".



```{r}
locationized_sample_data <- sample_data[['disaster_data']] %>%
  split_locations(column_name = "Location")
```

The locationized Data frame is compatible with exploratory functions like percent_located_locations and percent_located_disasters which allow for easy visualization of the coverage provided in the data, with respect to latitudes and longitudes.

### Percent Located Location-Disaster Pairings

This function displays what proportion of the locations (obtained by split_locations) have been geocoded in the input dataset.

```{r}
locationized_sample_data  %>%
  percent_located_locations(lat_column = "Latitude",
                            lng_column = "Longitude")
```

### Percent Located Disasters

This is quite self explanatory, in the sense that it displays what proportion of the disasters have been geocoded in the input dataset.

```{r}
locationized_sample_data  %>%
  percent_located_disasters(lat_column = "Latitude",
                            lng_column = "Longitude")
```

As we can see, the coverage is very sparse. Certainly not enough for proper analysis. 

Once the data has been locationized, it is ready to be geocoded.


### Geocoding

Weed uses the free [geonames API](https://www.geonames.org/) to geocode each location. To use this functionality, one must first create a free account and then supply their username to the geocode function in Weed. This function comes with a few options, depending on the kind of analysis that is being performed. Researchers can utilize the n_results parameter to get the n closest matches to the input location and decide which one to use. The unwrap parameter also allows researchers to keep the geocoded data in a nested Data frame structure, possibly good for exporting, or in unwrapped from, where each lat and long gets a separate column (lat1, lng1, lat2, lng2, etc.)

```{r username, include=FALSE}
sample_username = "rammkripa"
```

```{r geocode, results=FALSE}
geocoded_df <- locationized_df %>%
  geocode(unwrap = FALSE, geonames_username = sample_username)
```
```{r geocodeviz, echo=FALSE}
geocoded_df %>%
  head() %>%
  select(-c(Location, uncertain_location_specificity, `Disaster Type`)) %>%
  flextable() %>%
  add_header_row(values = c("Geocoded Data"), colwidths = c(7)) %>%
  bg(j = c("lat", "lng"), bg = "darkolivegreen1") %>%
  bg(j = c("Latitude", "Longitude"), bg = "cadetblue1")
```

If your dataset has more than 1000 records after locationizing it is advisable to use our function geocode_batches which includes a cooldown time between batches so that issues with limits on the number of queries one can make to the geonames API can be avoided.

Further exploration with the percent_located_locations and percent_located_disasters functions is advisable, to visualize the success of our geocoding. As the data is indeed locationized, a choice must be made as to how to decide if a disaster has been “located”. Two popular choices are any and all. “Any” considers a disaster located if any one of its constituent locations has valid lat-long data, while “all” requires every constituent location to be geocoded. These can be set by the how parameter of percent_located_disasters, which also allows user defined functions!

### Percent Located Locations



```{r locloc}
geocoded_df %>%
  percent_located_locations()
```

### Percent Located Disasters

A slight change from before as, now, we have different geocoding for each location within a disaster. We can decide whether to consider a disaster to be geocoded if any of the locations have been geocoded, or all, or some specific percentage or combination. "How" and "any" are default options but we support user-defined aggregation functions as well.

```{r locdiz}
geocoded_df %>%
  percent_located_disasters(how = "any")
```

The goal with the geocoding and subsequent exploration was to provide as much flexibility and modularity to this step of the workflow as possible, to allow for diverse analyses and use cases.

### Elementary Analysis

One of the most common uses of lat-long data is checking whether a point lies in some defined region. Weed allows for regions to be defined either as a lat-long box, or as a shapefile. For increased modularity, the shapefile may be defined as either a shape object or even as a string containing the file name.

#### Lat-Long Box

Assume the required box is given as follows

```{r}
tllat = 40 # top left latitude
tllng = -119 # top left longitude
brlat = 35 # bottom right latitude
brlng = -75 # bottom right longitude
```

We want to find out whether each of the disaster-location pairings in our dataset falls within this latitiude and longitude bounding box. This easy-to-use function performs the simple task, and can easily be piped with elementary visual analysis for verification and ease of understanding.

```{r}
inbox_df <- geocoded_df %>%
  located_in_box(top_left_lat = tllat, top_left_lng = tllng, bottom_right_lat = brlat, bottom_right_lng = brlng)
```

```{r inboxviz, echo=FALSE}
inbox_df %>%
  head() %>%
  select(-Location) %>%
  flextable() %>%
  bg(j = c('in_box'), bg = scales::col_factor(palette = c("green", "orange"), domain = c(TRUE, FALSE))) %>%
  add_header_row(values = c("Lat Long Box Data"), colwidths = c(10))
```


```{r, echo = FALSE}
map_of_us <- rnaturalearth::ne_countries(country = "United States of America",
                         returnclass = "sf")
map_of_canada <- rnaturalearth::ne_countries(country = "Canada",
                         returnclass = "sf")
map_of_mexico <- rnaturalearth::ne_countries(country = "Mexico",
                         returnclass = "sf")
inbox_df %>%
ggplot() +
  geom_sf(data = map_of_us) +
  geom_sf(data = map_of_canada) +
  geom_sf(data = map_of_mexico) +
  geom_rect(xmin = tllng, xmax = brlng, ymin = brlat, ymax = tllat, fill = "yellow") +
  geom_hline(yintercept = brlat, color = "yellow") +
  geom_hline(yintercept = tllat, color = "yellow") +
  geom_vline(xintercept = tllng, color = "yellow") +
  geom_vline(xintercept = brlng, color = "yellow") +
  geom_point(mapping = aes(x = lng, y = lat, color = in_box)) +
  xlim(-150, -40) +
  ylim(30, 70) +
  labs(title = "Result of the in_box function", subtitle = " ", x = "Longitude", y = "Latitude")
```

#### Shapefile

Likewise, we can check whether points lie in a given shapefile.
Here we see which points lie in the region of California.

```{r, echo = FALSE}
map_of_us <- rnaturalearth::ne_countries(country = "United States of America",
                         returnclass = "sf")
map_of_canada <- rnaturalearth::ne_countries(country = "Canada",
                         returnclass = "sf")
map_of_mexico <- rnaturalearth::ne_countries(country = "Mexico",
                         returnclass = "sf")
map_california <- USAboundaries::us_states(states = c('California'))
inbox_df %>%
  filter(!is.na(lat)) %>%
  located_in_shapefile(shapefile = map_california) %>%
  ggplot() +
  geom_sf(data = map_of_us) +
  geom_sf(data = map_of_canada) +
  geom_sf(data = map_of_mexico) +
  geom_sf(data = map_california, color = 'yellow', fill = 'yellow') +
  geom_point(mapping = aes(x = lng, y = lat, color = in_shape)) +
  labs(title = "Result of the in_shapefile function", subtitle = "Showing the disaster location points and the shapefile polygon", x = "Longitude", y = "Latitude") +
  xlim(-150, -40) +
  ylim(30, 70)
```

These functionalities should help with most common geocoding use cases and serve as a starting point for further analysis.

## Impact and Conclusion

The Wrangler for Emergency Events Database package can be used by researchers for analysis of disaster patterns in many regions. While it does not directly deal with analysis functions, the data cleaning and preparation tools offered by weed have proven useful in conducting further research. Geocoded disaster-location pairs are invaluable, and help connect the various locations affected by each calamity. Other efforts in this sphere of geocoding disaster data seem to be static, in that net new data is not geocoded, and seem to offer only one location for each disaster, even though common sense would suggest that many types of disasters affect a number of places in different ways. By changing the unit of analysis, providing geocoding and elemntary exploratory analysis tools, we hope that weed is eventually the first step of choice in emergency event analysis.


## Acknowledgements

## References